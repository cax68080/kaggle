{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f0aed38",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-10T08:01:23.095650Z",
     "iopub.status.busy": "2022-08-10T08:01:23.094994Z",
     "iopub.status.idle": "2022-08-10T08:01:25.013941Z",
     "shell.execute_reply": "2022-08-10T08:01:25.012876Z"
    },
    "papermill": {
     "duration": 1.930868,
     "end_time": "2022-08-10T08:01:25.016889",
     "exception": false,
     "start_time": "2022-08-10T08:01:23.086021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from decimal import Decimal as dec\n",
    "import datetime\n",
    "import time\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cfa8c05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:01:25.029021Z",
     "iopub.status.busy": "2022-08-10T08:01:25.028122Z",
     "iopub.status.idle": "2022-08-10T08:03:55.502758Z",
     "shell.execute_reply": "2022-08-10T08:03:55.501349Z"
    },
    "papermill": {
     "duration": 150.483354,
     "end_time": "2022-08-10T08:03:55.505332",
     "exception": false,
     "start_time": "2022-08-10T08:01:25.021978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 96.13 Mb (78.8% reduction)\n",
      "Mem. usage decreased to 143.53 Mb (31.2% reduction)\n",
      "Mem. usage decreased to  0.12 Mb (41.9% reduction)\n",
      "Mem. usage decreased to  2.09 Mb (84.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def load_data():\n",
    "    train_df = reduce_mem_usage(pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_evaluation.csv\"))\n",
    "    prices_df = reduce_mem_usage(pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\"))\n",
    "    calendar_df = reduce_mem_usage(pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\"))\n",
    "    submission_df = reduce_mem_usage(pd.read_csv(\"../input/m5-forecasting-accuracy/sample_submission.csv\"))\n",
    "    return train_df, prices_df, calendar_df, submission_df\n",
    "\n",
    "train_df, prices_df, calendar_df, submission_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bec9c988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:03:55.517047Z",
     "iopub.status.busy": "2022-08-10T08:03:55.516651Z",
     "iopub.status.idle": "2022-08-10T08:03:55.529030Z",
     "shell.execute_reply": "2022-08-10T08:03:55.527884Z"
    },
    "papermill": {
     "duration": 0.020974,
     "end_time": "2022-08-10T08:03:55.531298",
     "exception": false,
     "start_time": "2022-08-10T08:03:55.510324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_base_grid(train_df, end_train_day_x, predict_horizon):\n",
    "    index_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "\n",
    "    grid_df = pd.melt(train_df, id_vars=index_columns, var_name='d', value_name='sales')\n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "\n",
    "    grid_df['d_org'] = grid_df['d']\n",
    "    grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
    "\n",
    "    time_mask = (grid_df['d'] > end_train_day_x) &  (grid_df['d'] <= end_train_day_x + predict_horizon)\n",
    "    holdout_df = grid_df.loc[time_mask, [\"id\", \"d\", \"sales\"]].reset_index(drop=True)\n",
    "    holdout_df.to_feather(f\"holdout_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    del(holdout_df)\n",
    "    gc.collect()\n",
    "\n",
    "    grid_df = grid_df[grid_df['d'] <= end_train_day_x]\n",
    "    grid_df['d'] = grid_df['d_org']\n",
    "    grid_df = grid_df.drop('d_org', axis=1)\n",
    "\n",
    "    add_grid = pd.DataFrame()\n",
    "    for i in range(predict_horizon):\n",
    "        temp_df = train_df[index_columns]\n",
    "        temp_df = temp_df.drop_duplicates()\n",
    "        temp_df['d'] = 'd_' + str(end_train_day_x + i + 1)\n",
    "        temp_df['sales'] = np.nan\n",
    "        add_grid = pd.concat([add_grid, temp_df])\n",
    "    \n",
    "    grid_df = pd.concat([grid_df, add_grid])\n",
    "    grid_df = grid_df.reset_index(drop=True)\n",
    "    \n",
    "    for col in index_columns:\n",
    "        grid_df[col] = grid_df[col].astype('category')\n",
    "    \n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "    grid_df.to_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    del(grid_df)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef9dbf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:03:55.542973Z",
     "iopub.status.busy": "2022-08-10T08:03:55.542613Z",
     "iopub.status.idle": "2022-08-10T08:03:55.554270Z",
     "shell.execute_reply": "2022-08-10T08:03:55.553285Z"
    },
    "papermill": {
     "duration": 0.020392,
     "end_time": "2022-08-10T08:03:55.556686",
     "exception": false,
     "start_time": "2022-08-10T08:03:55.536294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1\n",
    "    \n",
    "def calc_release_week(prices_df, end_train_day_x, predict_horizon):\n",
    "    index_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "    \n",
    "    grid_df = pd.read_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    \n",
    "    release_df = prices_df.groupby(['store_id', 'item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
    "    release_df.columns = ['store_id', 'item_id', 'release']\n",
    "    \n",
    "    grid_df = merge_by_concat(grid_df, release_df, ['store_id', 'item_id'])\n",
    "    \n",
    "    del release_df\n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "    gc.collect()\n",
    "    \n",
    "    grid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk', 'd']], ['d'])\n",
    "    grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "    grid_df['release'] = grid_df['release'] - grid_df['release'].min()\n",
    "    grid_df['release'] = grid_df['release'].astype(np.int16)\n",
    "    \n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "    grid_df.to_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    del(grid_df)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e81d337a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:03:55.568084Z",
     "iopub.status.busy": "2022-08-10T08:03:55.567715Z",
     "iopub.status.idle": "2022-08-10T08:03:55.584494Z",
     "shell.execute_reply": "2022-08-10T08:03:55.583669Z"
    },
    "papermill": {
     "duration": 0.025189,
     "end_time": "2022-08-10T08:03:55.586836",
     "exception": false,
     "start_time": "2022-08-10T08:03:55.561647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_grid_price(prices_df, calendar_df, end_train_day_x, predict_horizon):\n",
    "\n",
    "    grid_df = pd.read_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "\n",
    "    prices_df['price_max'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('max')\n",
    "    prices_df['price_min'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('min')\n",
    "    prices_df['price_std'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('std')\n",
    "    prices_df['price_mean'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('mean')\n",
    "    prices_df['price_norm'] = prices_df['sell_price'] / prices_df['price_max']\n",
    "    prices_df['price_nunique'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('nunique')\n",
    "    prices_df['item_nunique'] = prices_df.groupby(['store_id', 'sell_price'])['item_id'].transform('nunique')\n",
    "\n",
    "    calendar_prices = calendar_df[['wm_yr_wk', 'month', 'year']]\n",
    "    calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "    prices_df = prices_df.merge(calendar_prices[['wm_yr_wk', 'month', 'year']], on=['wm_yr_wk'], how='left')\n",
    "    \n",
    "    del calendar_prices\n",
    "    gc.collect()\n",
    "    \n",
    "    prices_df['price_momentum'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id'])[\n",
    "        'sell_price'].transform(lambda x: x.shift(1))\n",
    "    prices_df['price_momentum_m'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id', 'month'])[\n",
    "        'sell_price'].transform('mean')\n",
    "    prices_df['price_momentum_y'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id', 'year'])[\n",
    "        'sell_price'].transform('mean')\n",
    "\n",
    "    prices_df['sell_price_cent'] = [math.modf(p)[0] for p in prices_df['sell_price']]\n",
    "    prices_df['price_max_cent'] = [math.modf(p)[0] for p in prices_df['price_max']]\n",
    "    prices_df['price_min_cent'] = [math.modf(p)[0] for p in prices_df['price_min']]\n",
    "\n",
    "    del prices_df['month'], prices_df['year']\n",
    "    prices_df = reduce_mem_usage(prices_df, verbose=False)\n",
    "    gc.collect()\n",
    "    \n",
    "    original_columns = list(grid_df)\n",
    "    grid_df = grid_df.merge(prices_df, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "    del(prices_df)\n",
    "    gc.collect()\n",
    "    \n",
    "    keep_columns = [col for col in list(grid_df) if col not in original_columns]\n",
    "    grid_df = grid_df[['id', 'd'] + keep_columns]\n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "\n",
    "    grid_df.to_feather(f\"grid_price_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    del(grid_df)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "797011fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:03:55.598422Z",
     "iopub.status.busy": "2022-08-10T08:03:55.597996Z",
     "iopub.status.idle": "2022-08-10T08:03:55.613575Z",
     "shell.execute_reply": "2022-08-10T08:03:55.612606Z"
    },
    "papermill": {
     "duration": 0.024182,
     "end_time": "2022-08-10T08:03:55.615882",
     "exception": false,
     "start_time": "2022-08-10T08:03:55.591700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_moon_phase(d):  # 0=new, 4=full; 4 days/phase\n",
    "    diff = datetime.datetime.strptime(d, '%Y-%m-%d') - datetime.datetime(2001, 1, 1)\n",
    "    days = dec(diff.days) + (dec(diff.seconds) / dec(86400))\n",
    "    lunations = dec(\"0.20439731\") + (days * dec(\"0.03386319269\"))\n",
    "    phase_index = math.floor((lunations % dec(1) * dec(8)) + dec('0.5'))\n",
    "    return int(phase_index) & 7\n",
    "    \n",
    "def generate_grid_calendar(calendar_df, end_train_day_x, predict_horizon):\n",
    "    \n",
    "    grid_df = pd.read_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    grid_df = grid_df[['id', 'd']]\n",
    "    gc.collect()\n",
    "\n",
    "    calendar_df['moon'] = calendar_df.date.apply(get_moon_phase)\n",
    "\n",
    "    # Merge calendar partly\n",
    "    icols = ['date',\n",
    "             'd',\n",
    "             'event_name_1',\n",
    "             'event_type_1',\n",
    "             'event_name_2',\n",
    "             'event_type_2',\n",
    "             'snap_CA',\n",
    "             'snap_TX',\n",
    "             'snap_WI',\n",
    "             'moon',\n",
    "             ]\n",
    "\n",
    "    grid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')\n",
    "\n",
    "    icols = ['event_name_1',\n",
    "             'event_type_1',\n",
    "             'event_name_2',\n",
    "             'event_type_2',\n",
    "             'snap_CA',\n",
    "             'snap_TX',\n",
    "             'snap_WI']\n",
    "    \n",
    "    for col in icols:\n",
    "        grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "    grid_df['date'] = pd.to_datetime(grid_df['date'])\n",
    "\n",
    "    grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\n",
    "    grid_df['tm_w'] = grid_df['date'].dt.isocalendar().week.astype(np.int8)\n",
    "    grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\n",
    "    grid_df['tm_y'] = grid_df['date'].dt.year\n",
    "    grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\n",
    "    grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: math.ceil(x / 7)).astype(np.int8)\n",
    "\n",
    "    grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)\n",
    "    grid_df['tm_w_end'] = (grid_df['tm_dw'] >= 5).astype(np.int8)\n",
    "                                                         \n",
    "    del(grid_df['date'])\n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "    grid_df.to_feather(f\"grid_calendar_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "                                                         \n",
    "    del(grid_df)\n",
    "    del(calendar_df)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46aafb19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:03:55.627220Z",
     "iopub.status.busy": "2022-08-10T08:03:55.626614Z",
     "iopub.status.idle": "2022-08-10T08:03:55.633174Z",
     "shell.execute_reply": "2022-08-10T08:03:55.631992Z"
    },
    "papermill": {
     "duration": 0.014757,
     "end_time": "2022-08-10T08:03:55.635489",
     "exception": false,
     "start_time": "2022-08-10T08:03:55.620732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modify_grid_base(end_train_day_x, predict_horizon):\n",
    "    grid_df = pd.read_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
    "    del grid_df['wm_yr_wk']\n",
    "    \n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "    grid_df.to_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    \n",
    "    del(grid_df)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0b03905",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:03:55.647111Z",
     "iopub.status.busy": "2022-08-10T08:03:55.646705Z",
     "iopub.status.idle": "2022-08-10T08:03:55.658414Z",
     "shell.execute_reply": "2022-08-10T08:03:55.657189Z"
    },
    "papermill": {
     "duration": 0.020351,
     "end_time": "2022-08-10T08:03:55.660674",
     "exception": false,
     "start_time": "2022-08-10T08:03:55.640323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_lag_feature(end_train_day_x, predict_horizon):\n",
    "    grid_df = pd.read_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    grid_df = grid_df[['id', 'd', 'sales']]\n",
    "    \n",
    "    num_lag_day_list = []\n",
    "    num_lag_day = 15\n",
    "    for col in range(predict_horizon, predict_horizon + num_lag_day):\n",
    "        num_lag_day_list.append(col)\n",
    "    \n",
    "    num_rolling_day_list = [7, 14, 30, 60, 180]\n",
    "    num_shift_rolling_day_list = []\n",
    "    for num_shift_day in [1, 7, 14]:\n",
    "        for num_rolling_day in [7, 14, 30, 60]:\n",
    "            num_shift_rolling_day_list.append([num_shift_day, num_rolling_day])\n",
    "   \n",
    "    grid_df = grid_df.assign(**{\n",
    "        '{}_lag_{}'.format(col, l): grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(l))\n",
    "        for l in num_lag_day_list\n",
    "    })\n",
    "\n",
    "    for col in list(grid_df):\n",
    "        if 'lag' in col:\n",
    "            grid_df[col] = grid_df[col].astype(np.float16)\n",
    "\n",
    "    for num_rolling_day in num_rolling_day_list:\n",
    "        grid_df['rolling_mean_' + str(num_rolling_day)] = grid_df.groupby(['id'])['sales'].transform(\n",
    "            lambda x: x.shift(predict_horizon).rolling(num_rolling_day).mean()).astype(np.float16)\n",
    "        grid_df['rolling_std_' + str(num_rolling_day)] = grid_df.groupby(['id'])['sales'].transform(\n",
    "            lambda x: x.shift(predict_horizon).rolling(num_rolling_day).std()).astype(np.float16)\n",
    "\n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "    grid_df.to_feather(f\"lag_feature_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    \n",
    "    del(grid_df)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "427daabc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:03:55.672273Z",
     "iopub.status.busy": "2022-08-10T08:03:55.671583Z",
     "iopub.status.idle": "2022-08-10T08:03:55.685534Z",
     "shell.execute_reply": "2022-08-10T08:03:55.684165Z"
    },
    "papermill": {
     "duration": 0.022712,
     "end_time": "2022-08-10T08:03:55.688293",
     "exception": false,
     "start_time": "2022-08-10T08:03:55.665581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_target_encoding_feature(end_train_day_x, predict_horizon):\n",
    "\n",
    "    grid_df = pd.read_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    \n",
    "    grid_df.loc[grid_df['d'] > (end_train_day_x - predict_horizon), 'sales'] = np.nan\n",
    "    base_cols = list(grid_df)\n",
    "\n",
    "    icols = [\n",
    "        ['state_id'],\n",
    "        ['store_id'],\n",
    "        ['cat_id'],\n",
    "        ['dept_id'],\n",
    "        ['state_id', 'cat_id'],\n",
    "        ['state_id', 'dept_id'],\n",
    "        ['store_id', 'cat_id'],\n",
    "        ['store_id', 'dept_id'],\n",
    "        ['item_id'],\n",
    "        ['item_id', 'state_id'],\n",
    "        ['item_id', 'store_id']\n",
    "    ]\n",
    "\n",
    "    for col in icols:\n",
    "        col_name = '_' + '_'.join(col) + '_'\n",
    "        grid_df['enc' + col_name + 'mean'] = grid_df.groupby(col)['sales'].transform('mean').astype(\n",
    "            np.float16)\n",
    "        grid_df['enc' + col_name + 'std'] = grid_df.groupby(col)['sales'].transform('std').astype(\n",
    "            np.float16)\n",
    "\n",
    "    keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
    "    grid_df = grid_df[['id', 'd'] + keep_cols]\n",
    "\n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "    grid_df.to_feather(f\"target_encoding_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    \n",
    "    del(grid_df)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e0eb2c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:03:55.700427Z",
     "iopub.status.busy": "2022-08-10T08:03:55.699920Z",
     "iopub.status.idle": "2022-08-10T08:03:55.718187Z",
     "shell.execute_reply": "2022-08-10T08:03:55.716966Z"
    },
    "papermill": {
     "duration": 0.027689,
     "end_time": "2022-08-10T08:03:55.721074",
     "exception": false,
     "start_time": "2022-08-10T08:03:55.693385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assemble_grid_by_store(train_df, end_train_day_x, predict_horizon):\n",
    "    grid_df = pd.concat([pd.read_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\"),\n",
    "                     pd.read_feather(f\"grid_price_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\").iloc[:, 2:],\n",
    "                     pd.read_feather(f\"grid_calendar_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\").iloc[:, 2:]],\n",
    "                     axis=1)\n",
    "    gc.collect()\n",
    "    store_id_set_list = list(train_df['store_id'].unique())\n",
    "\n",
    "    index_store = dict()\n",
    "    for store_id in store_id_set_list:\n",
    "        extract = grid_df[grid_df['store_id'] == store_id]\n",
    "        index_store[store_id] = extract.index.to_numpy()\n",
    "        extract = extract.reset_index(drop=True)\n",
    "        extract.to_feather(f\"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "\n",
    "    del(grid_df)\n",
    "    gc.collect()\n",
    "    \n",
    "    mean_features = [\n",
    "        'enc_cat_id_mean', 'enc_cat_id_std',\n",
    "        'enc_dept_id_mean', 'enc_dept_id_std',\n",
    "        'enc_item_id_mean', 'enc_item_id_std'\n",
    "        ]\n",
    "    df2 = pd.read_feather(f\"target_encoding_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")[mean_features]\n",
    "\n",
    "    for store_id in store_id_set_list:\n",
    "        df = pd.read_feather(f\"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "        df = pd.concat([df, df2[df2.index.isin(index_store[store_id])].reset_index(drop=True)], axis=1)\n",
    "        df.to_feather(f\"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "\n",
    "    del(df2)\n",
    "    gc.collect()\n",
    "    \n",
    "    df3 = pd.read_feather(f\"lag_feature_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\").iloc[:, 3:]\n",
    "\n",
    "    for store_id in store_id_set_list:\n",
    "        df = pd.read_feather(f\"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "        df = pd.concat([df, df3[df3.index.isin(index_store[store_id])].reset_index(drop=True)], axis=1)\n",
    "        df.to_feather(f\"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "\n",
    "    del(df3)\n",
    "    del(store_id_set_list)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36f53633",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:03:55.733239Z",
     "iopub.status.busy": "2022-08-10T08:03:55.732765Z",
     "iopub.status.idle": "2022-08-10T08:03:55.743051Z",
     "shell.execute_reply": "2022-08-10T08:03:55.741777Z"
    },
    "papermill": {
     "duration": 0.019406,
     "end_time": "2022-08-10T08:03:55.745859",
     "exception": false,
     "start_time": "2022-08-10T08:03:55.726453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_grid_by_store(end_train_day_x, predict_horizon, store_id):\n",
    "    df = pd.read_feather(f\"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "                          \n",
    "    remove_features = ['id', 'state_id', 'store_id', 'date', 'wm_yr_wk', 'd', 'sales']\n",
    "    enable_features = [col for col in list(df) if col not in remove_features]\n",
    "    df = df[['id', 'd', 'sales'] + enable_features]\n",
    "    df = reduce_mem_usage(df, verbose=False)\n",
    "    gc.collect()\n",
    "                          \n",
    "    return df, enable_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c2d9c62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:03:55.762473Z",
     "iopub.status.busy": "2022-08-10T08:03:55.761230Z",
     "iopub.status.idle": "2022-08-10T08:03:55.779150Z",
     "shell.execute_reply": "2022-08-10T08:03:55.778113Z"
    },
    "papermill": {
     "duration": 0.029563,
     "end_time": "2022-08-10T08:03:55.782791",
     "exception": false,
     "start_time": "2022-08-10T08:03:55.753228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(train_df, seed, end_train_day_x, predict_horizon):\n",
    "    \n",
    "    lgb_params = {\n",
    "            'boosting_type': 'goss',\n",
    "            'objective': 'tweedie',\n",
    "            'tweedie_variance_power': 1.1,\n",
    "            'metric': 'rmse',\n",
    "             #'subsample': 0.5,\n",
    "             #'subsample_freq': 1,\n",
    "            'learning_rate': 0.03,\n",
    "            'num_leaves': 2 ** 11 - 1,\n",
    "            'min_data_in_leaf': 2 ** 12 - 1,\n",
    "            'feature_fraction': 0.5,\n",
    "            'max_bin': 100,\n",
    "            'boost_from_average': False,\n",
    "            'num_boost_round': 1400,\n",
    "            'verbose': -1,\n",
    "            'num_threads': os.cpu_count(),\n",
    "            'force_row_wise': True,\n",
    "        }\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    lgb_params['seed'] = seed\n",
    "\n",
    "    store_id_set_list = list(train_df['store_id'].unique())\n",
    "    print(f\"training stores: {store_id_set_list}\")\n",
    "    \n",
    "    feature_importance_all_df = pd.DataFrame()\n",
    "    for store_index, store_id in enumerate(store_id_set_list):\n",
    "        print(f'now training {store_id} store')\n",
    "\n",
    "        grid_df, enable_features = load_grid_by_store(end_train_day_x, predict_horizon, store_id)\n",
    "\n",
    "        train_mask = grid_df['d'] <= end_train_day_x\n",
    "        valid_mask = train_mask & (grid_df['d'] > (end_train_day_x - predict_horizon))\n",
    "        preds_mask = grid_df['d'] > (end_train_day_x - 100)\n",
    "\n",
    "        train_data = lgb.Dataset(grid_df[train_mask][enable_features],\n",
    "                                 label=grid_df[train_mask]['sales'])\n",
    "\n",
    "        valid_data = lgb.Dataset(grid_df[valid_mask][enable_features],\n",
    "                                 label=grid_df[valid_mask]['sales'])\n",
    "\n",
    "\n",
    "        # Saving part of the dataset for later predictions\n",
    "        # Removing features that we need to calculate recursively\n",
    "        grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "        grid_df.to_feather(f'test_{store_id}_{predict_horizon}.feather')\n",
    "        del(grid_df)\n",
    "        gc.collect()\n",
    "        \n",
    "        estimator = lgb.train(lgb_params,\n",
    "                              train_data,\n",
    "                              valid_sets=[valid_data],\n",
    "                              callbacks=[lgb.log_evaluation(period=100, show_stdv=False)],\n",
    "                              )\n",
    "\n",
    "        model_name = str(f'lgb_model_{store_id}_{predict_horizon}.bin')\n",
    "        feature_importance_store_df = pd.DataFrame(sorted(zip(enable_features, estimator.feature_importance())),\n",
    "                                                   columns=['feature_name', 'importance'])\n",
    "        feature_importance_store_df = feature_importance_store_df.sort_values('importance', ascending=False)\n",
    "        feature_importance_store_df['store_id'] = store_id\n",
    "        feature_importance_store_df.to_csv(f'feature_importance_{store_id}_{predict_horizon}.csv', index=False)\n",
    "        feature_importance_all_df = pd.concat([feature_importance_all_df, feature_importance_store_df])\n",
    "        pickle.dump(estimator, open(model_name, 'wb'))\n",
    "\n",
    "        del([train_data, valid_data, estimator])\n",
    "        gc.collect()\n",
    "\n",
    "    feature_importance_all_df.to_csv(f'feature_importance_all_{predict_horizon}.csv', index=False)\n",
    "    feature_importance_agg_df = feature_importance_all_df.groupby(\n",
    "        'feature_name')['importance'].agg(['mean', 'std']).reset_index()\n",
    "    feature_importance_agg_df.columns = ['feature_name', 'importance_mean', 'importance_std']\n",
    "    feature_importance_agg_df = feature_importance_agg_df.sort_values('importance_mean', ascending=False)\n",
    "    feature_importance_agg_df.to_csv(f'feature_importance_agg_{predict_horizon}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c89da36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:03:55.798116Z",
     "iopub.status.busy": "2022-08-10T08:03:55.797747Z",
     "iopub.status.idle": "2022-08-10T08:03:55.805494Z",
     "shell.execute_reply": "2022-08-10T08:03:55.804511Z"
    },
    "papermill": {
     "duration": 0.018009,
     "end_time": "2022-08-10T08:03:55.807990",
     "exception": false,
     "start_time": "2022-08-10T08:03:55.789981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_pipeline(train_df, prices_df, calendar_df, end_train_day_x_list, prediction_horizon_list):\n",
    "    \n",
    "    for end_train_day_x in end_train_day_x_list:\n",
    "        \n",
    "        for predict_horizon in prediction_horizon_list:\n",
    "            \n",
    "            print(f\"end training point day: {end_train_day_x} - prediction horizon: {predict_horizon} days\")\n",
    "\n",
    "            # Data preparation\n",
    "            generate_base_grid(train_df, end_train_day_x, predict_horizon)\n",
    "            calc_release_week(prices_df, end_train_day_x, predict_horizon)\n",
    "            generate_grid_price(prices_df, calendar_df, end_train_day_x, predict_horizon)\n",
    "            generate_grid_calendar(calendar_df, end_train_day_x, predict_horizon)\n",
    "            modify_grid_base(end_train_day_x, predict_horizon)\n",
    "            generate_lag_feature(end_train_day_x, predict_horizon)\n",
    "            generate_target_encoding_feature(end_train_day_x, predict_horizon)\n",
    "            assemble_grid_by_store(train_df, end_train_day_x, predict_horizon)\n",
    "\n",
    "            # Modelling\n",
    "            train(train_df, seed, end_train_day_x, predict_horizon)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9500be95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:03:55.819537Z",
     "iopub.status.busy": "2022-08-10T08:03:55.818621Z",
     "iopub.status.idle": "2022-08-10T13:52:44.873930Z",
     "shell.execute_reply": "2022-08-10T13:52:44.871684Z"
    },
    "papermill": {
     "duration": 20929.065393,
     "end_time": "2022-08-10T13:52:44.878213",
     "exception": false,
     "start_time": "2022-08-10T08:03:55.812820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end training point day: 1913 - prediction horizon: 21 days\n",
      "training stores: ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n",
      "now training CA_1 store\n",
      "[100]\tvalid_0's rmse: 2.05886\n",
      "[200]\tvalid_0's rmse: 2.05297\n",
      "[300]\tvalid_0's rmse: 2.03962\n",
      "[400]\tvalid_0's rmse: 2.02675\n",
      "[500]\tvalid_0's rmse: 2.01437\n",
      "[600]\tvalid_0's rmse: 2.00444\n",
      "[700]\tvalid_0's rmse: 1.99396\n",
      "[800]\tvalid_0's rmse: 1.98501\n",
      "[900]\tvalid_0's rmse: 1.97702\n",
      "[1000]\tvalid_0's rmse: 1.96914\n",
      "[1100]\tvalid_0's rmse: 1.9621\n",
      "[1200]\tvalid_0's rmse: 1.95445\n",
      "[1300]\tvalid_0's rmse: 1.94783\n",
      "[1400]\tvalid_0's rmse: 1.94179\n",
      "now training CA_2 store\n",
      "[100]\tvalid_0's rmse: 2.00262\n",
      "[200]\tvalid_0's rmse: 1.93369\n",
      "[300]\tvalid_0's rmse: 1.89588\n",
      "[400]\tvalid_0's rmse: 1.86985\n",
      "[500]\tvalid_0's rmse: 1.85208\n",
      "[600]\tvalid_0's rmse: 1.83816\n",
      "[700]\tvalid_0's rmse: 1.82679\n",
      "[800]\tvalid_0's rmse: 1.81764\n",
      "[900]\tvalid_0's rmse: 1.80911\n",
      "[1000]\tvalid_0's rmse: 1.80139\n",
      "[1100]\tvalid_0's rmse: 1.79453\n",
      "[1200]\tvalid_0's rmse: 1.78761\n",
      "[1300]\tvalid_0's rmse: 1.78122\n",
      "[1400]\tvalid_0's rmse: 1.77537\n",
      "now training CA_3 store\n",
      "[100]\tvalid_0's rmse: 2.61577\n",
      "[200]\tvalid_0's rmse: 2.55065\n",
      "[300]\tvalid_0's rmse: 2.51613\n",
      "[400]\tvalid_0's rmse: 2.49427\n",
      "[500]\tvalid_0's rmse: 2.47676\n",
      "[600]\tvalid_0's rmse: 2.46053\n",
      "[700]\tvalid_0's rmse: 2.44624\n",
      "[800]\tvalid_0's rmse: 2.43543\n",
      "[900]\tvalid_0's rmse: 2.42439\n",
      "[1000]\tvalid_0's rmse: 2.41458\n",
      "[1100]\tvalid_0's rmse: 2.40573\n",
      "[1200]\tvalid_0's rmse: 2.39623\n",
      "[1300]\tvalid_0's rmse: 2.38863\n",
      "[1400]\tvalid_0's rmse: 2.38079\n",
      "now training CA_4 store\n",
      "[100]\tvalid_0's rmse: 1.35702\n",
      "[200]\tvalid_0's rmse: 1.34534\n",
      "[300]\tvalid_0's rmse: 1.33624\n",
      "[400]\tvalid_0's rmse: 1.32937\n",
      "[500]\tvalid_0's rmse: 1.32424\n",
      "[600]\tvalid_0's rmse: 1.3194\n",
      "[700]\tvalid_0's rmse: 1.31523\n",
      "[800]\tvalid_0's rmse: 1.31107\n",
      "[900]\tvalid_0's rmse: 1.30754\n",
      "[1000]\tvalid_0's rmse: 1.30456\n",
      "[1100]\tvalid_0's rmse: 1.30156\n",
      "[1200]\tvalid_0's rmse: 1.29862\n",
      "[1300]\tvalid_0's rmse: 1.29564\n",
      "[1400]\tvalid_0's rmse: 1.29268\n",
      "now training TX_1 store\n",
      "[100]\tvalid_0's rmse: 1.7224\n",
      "[200]\tvalid_0's rmse: 1.69351\n",
      "[300]\tvalid_0's rmse: 1.67785\n",
      "[400]\tvalid_0's rmse: 1.66442\n",
      "[500]\tvalid_0's rmse: 1.6502\n",
      "[600]\tvalid_0's rmse: 1.6394\n",
      "[700]\tvalid_0's rmse: 1.62833\n",
      "[800]\tvalid_0's rmse: 1.62092\n",
      "[900]\tvalid_0's rmse: 1.61289\n",
      "[1000]\tvalid_0's rmse: 1.60554\n",
      "[1100]\tvalid_0's rmse: 1.59797\n",
      "[1200]\tvalid_0's rmse: 1.59135\n",
      "[1300]\tvalid_0's rmse: 1.58474\n",
      "[1400]\tvalid_0's rmse: 1.57867\n",
      "now training TX_2 store\n",
      "[100]\tvalid_0's rmse: 1.84206\n",
      "[200]\tvalid_0's rmse: 1.82717\n",
      "[300]\tvalid_0's rmse: 1.80632\n",
      "[400]\tvalid_0's rmse: 1.79138\n",
      "[500]\tvalid_0's rmse: 1.77931\n",
      "[600]\tvalid_0's rmse: 1.76765\n",
      "[700]\tvalid_0's rmse: 1.75565\n",
      "[800]\tvalid_0's rmse: 1.74677\n",
      "[900]\tvalid_0's rmse: 1.73824\n",
      "[1000]\tvalid_0's rmse: 1.72909\n",
      "[1100]\tvalid_0's rmse: 1.72351\n",
      "[1200]\tvalid_0's rmse: 1.71646\n",
      "[1300]\tvalid_0's rmse: 1.71031\n",
      "[1400]\tvalid_0's rmse: 1.70454\n",
      "now training TX_3 store\n",
      "[100]\tvalid_0's rmse: 1.80558\n",
      "[200]\tvalid_0's rmse: 1.79548\n",
      "[300]\tvalid_0's rmse: 1.77891\n",
      "[400]\tvalid_0's rmse: 1.76513\n",
      "[500]\tvalid_0's rmse: 1.75189\n",
      "[600]\tvalid_0's rmse: 1.73836\n",
      "[700]\tvalid_0's rmse: 1.72669\n",
      "[800]\tvalid_0's rmse: 1.71635\n",
      "[900]\tvalid_0's rmse: 1.70819\n",
      "[1000]\tvalid_0's rmse: 1.69986\n",
      "[1100]\tvalid_0's rmse: 1.69287\n",
      "[1200]\tvalid_0's rmse: 1.68695\n",
      "[1300]\tvalid_0's rmse: 1.68144\n",
      "[1400]\tvalid_0's rmse: 1.6752\n",
      "now training WI_1 store\n",
      "[100]\tvalid_0's rmse: 1.65525\n",
      "[200]\tvalid_0's rmse: 1.63111\n",
      "[300]\tvalid_0's rmse: 1.61572\n",
      "[400]\tvalid_0's rmse: 1.60439\n",
      "[500]\tvalid_0's rmse: 1.59502\n",
      "[600]\tvalid_0's rmse: 1.58763\n",
      "[700]\tvalid_0's rmse: 1.58094\n",
      "[800]\tvalid_0's rmse: 1.57471\n",
      "[900]\tvalid_0's rmse: 1.56875\n",
      "[1000]\tvalid_0's rmse: 1.56338\n",
      "[1100]\tvalid_0's rmse: 1.55826\n",
      "[1200]\tvalid_0's rmse: 1.55342\n",
      "[1300]\tvalid_0's rmse: 1.54908\n",
      "[1400]\tvalid_0's rmse: 1.54507\n",
      "now training WI_2 store\n",
      "[100]\tvalid_0's rmse: 3.00975\n",
      "[200]\tvalid_0's rmse: 2.86349\n",
      "[300]\tvalid_0's rmse: 2.79736\n",
      "[400]\tvalid_0's rmse: 2.74597\n",
      "[500]\tvalid_0's rmse: 2.70749\n",
      "[600]\tvalid_0's rmse: 2.67538\n",
      "[700]\tvalid_0's rmse: 2.64752\n",
      "[800]\tvalid_0's rmse: 2.62023\n",
      "[900]\tvalid_0's rmse: 2.59988\n",
      "[1000]\tvalid_0's rmse: 2.58117\n",
      "[1100]\tvalid_0's rmse: 2.56534\n",
      "[1200]\tvalid_0's rmse: 2.55188\n",
      "[1300]\tvalid_0's rmse: 2.53721\n",
      "[1400]\tvalid_0's rmse: 2.5238\n",
      "now training WI_3 store\n",
      "[100]\tvalid_0's rmse: 2.05148\n",
      "[200]\tvalid_0's rmse: 1.96664\n",
      "[300]\tvalid_0's rmse: 1.9367\n",
      "[400]\tvalid_0's rmse: 1.91289\n",
      "[500]\tvalid_0's rmse: 1.89433\n",
      "[600]\tvalid_0's rmse: 1.87742\n",
      "[700]\tvalid_0's rmse: 1.86748\n",
      "[800]\tvalid_0's rmse: 1.85637\n",
      "[900]\tvalid_0's rmse: 1.84368\n",
      "[1000]\tvalid_0's rmse: 1.83437\n",
      "[1100]\tvalid_0's rmse: 1.82563\n",
      "[1200]\tvalid_0's rmse: 1.81757\n",
      "[1300]\tvalid_0's rmse: 1.80917\n",
      "[1400]\tvalid_0's rmse: 1.80215\n"
     ]
    }
   ],
   "source": [
    "end_train_day_x_list = [1913] # [1941, 1913, 1885, 1857, 1829, 1577]\n",
    "prediction_horizon_list = [21] # [7, 14, 21, 28]\n",
    "seed = 42\n",
    "\n",
    "train_pipeline(train_df, prices_df, calendar_df, end_train_day_x_list, prediction_horizon_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21093.953862,
   "end_time": "2022-08-10T13:52:47.892835",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-10T08:01:13.938973",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
